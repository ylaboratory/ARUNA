{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145a0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/js228/ARUNA\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b98f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39f1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aruna.data_utils import get_mslice_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from aruna.models import DCAE_MSLICE\n",
    "from aruna.model_utils import get_peObj\n",
    "from aruna.model_engine import train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e75165",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd()\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# path where trained model will be saved\n",
    "model_fpath = os.path.join(CWD, \"checkpoints\", \n",
    "                           f\"trained_model_{timestamp}.pth\")\n",
    "# config path\n",
    "config_fpath = os.path.join(CWD, \"configs\", \"example_config.yaml\")\n",
    "\n",
    "# get sample names that need to be imputed, in our case all samples in original dir\n",
    "base_dir = Path(CWD) / \"data\" / \"gtex_subset\"\n",
    "sample_names = sorted(d.name for d in base_dir.iterdir() if d.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c539bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "with open(config_fpath) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "chr = config[\"data\"][\"chrom\"]\n",
    "batch_dim = config[\"model\"][\"batch_dim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb7e20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Getting Patch Data for:\n",
      "Dataset: gtex\n",
      "Chr(s): chr21\n",
      "Patch Type: mpatch\n",
      "NR: mcar_90\n",
      "#Samples: 16\n",
      "Current chromosome:  chr21\n",
      "Looking for Ground-Truth Patchified FM files...\n",
      "/home/js228/ARUNA/data/gtex/patch_centric/numCpg128/true/FractionalMethylation/chr21_patches.fm.pkl\n",
      "Patchified data found at: /home/js228/ARUNA/data/gtex/patch_centric/numCpg128/true/FractionalMethylation/chr21_patches.fm.pkl\n",
      "Curent chromosome:  chr21\n",
      "Looking for Patchified Noise-Simulated FractionalMethylation and Coverage files...\n",
      "All files exist!\n",
      "FM at: /home/js228/ARUNA/data/gtex/patch_centric/numCpg128/mcar_90/FractionalMethylation/chr21_patches.mask.fm.pkl\n",
      "MASK at: /home/js228/ARUNA/data/gtex/patch_centric/numCpg128/mcar_90/SimulatedMask/chr21_patches.mask.pkl\n",
      "Loaded Data with 16 samples and 577920 patches.\n",
      "#Batches in Training (batch_dim=256): 2258\n"
     ]
    }
   ],
   "source": [
    "trainData_obj = get_mslice_dataset(config, \n",
    "                                   samples = sample_names,\n",
    "                                   mode = \"train\")\n",
    "# initialize dataloaders\n",
    "trainloader = DataLoader(trainData_obj, \n",
    "                         batch_size = config[\"model\"][\"batch_dim\"], \n",
    "                         shuffle = True, num_workers = 4)\n",
    "print(\"#Batches in Training (batch_dim={}): {}\".format(config[\"model\"][\"batch_dim\"], \n",
    "                                                       len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a2a89fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model based on config...\n",
      "Model initialized and loaded onto GPU!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing model based on config...\")\n",
    "model = DCAE_MSLICE(config = config[\"model\"])\n",
    "\n",
    "assert config[\"model\"][\"posn_embed\"] == 'type1_concat', \"Alternate PE is currently not supported\"\n",
    "pe_type = config[\"model\"][\"posn_embed\"].split(\"_\")[0]\n",
    "\n",
    "embed_dim = None\n",
    "pe_obj = get_peObj(pe_type = pe_type, \n",
    "                    num_cpg = config[\"data\"][\"num_cpgs\"], \n",
    "                    embed_dim = None,\n",
    "                    chrom = chr)\n",
    "trainData_obj.pe_obj = pe_obj\n",
    "\n",
    "device = config[\"model\"][\"device\"]\n",
    "criterion = config[\"model\"][\"criterion\"]\n",
    "num_epochs = config[\"model\"][\"num_epochs\"]\n",
    "model = model.to(device)\n",
    "print(\"Model initialized and loaded onto GPU!\")\n",
    "\n",
    "assert criterion == \"mse\", \"Alternate loss functins are currently not supported\"\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "129276aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), \n",
    "                            lr = config[\"model\"][\"learning_rate\"], \n",
    "                            weight_decay = config[\"model\"][\"l2_penalty\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe5b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "0/2258 batches complete\n",
      "800/2258 batches complete\n",
      "1600/2258 batches complete\n",
      "2258/2258 batches complete\n",
      "Epoch Train 1/100 Loss: 0.05905976460409751\n",
      "Epoch 2/100\n",
      "0/2258 batches complete\n",
      "800/2258 batches complete\n",
      "1600/2258 batches complete\n",
      "2258/2258 batches complete\n",
      "Epoch Train 2/100 Loss: 0.05305686353171747\n",
      "Epoch 3/100\n",
      "0/2258 batches complete\n",
      "800/2258 batches complete\n",
      "1600/2258 batches complete\n",
      "2258/2258 batches complete\n",
      "Epoch Train 3/100 Loss: 0.04941611934161651\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): # replace with num_epochs\n",
    "    print(\"Epoch {}/{}\".format(epoch+1, num_epochs))\n",
    "\n",
    "    # Train 1 Epoch        \n",
    "    epoch_trainLoss = train_step(model, trainloader, \n",
    "                                 loss_fn, optimizer, device)\n",
    "    print(\"Epoch Train {}/{} Loss: {}\".format(epoch+1, num_epochs, epoch_trainLoss))\n",
    "                  \n",
    "torch.save(model.state_dict(), model_fpath)\n",
    "print(\"Training Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
